{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sun Apr 19 22:17:19 2020\n",
    "\n",
    "@author: Jonas\n",
    "\"\"\"\n",
    "#Soft Baseline\n",
    "import fasttext\n",
    "\n",
    "#General\n",
    "import pandas as pd\n",
    "\n",
    "#Hard Baseline library\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#NNModel library\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load already trained fasttext model an predit\n",
    "\n",
    "#model = fasttext.train_supervised(r\"C:\\Users\\Jonas\\Documents\\GSOM\\NLP\\train.ft.txt\",lr=1.0, epoch=3, wordNgrams=2)\n",
    "#model.save_model(\"model_fasttext_classification.bin\")  \n",
    "\n",
    "model=fasttext.load_model(\"model_fasttext_classification.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t400000\n",
      "P@1\t0.938\n",
      "R@1\t0.938\n"
     ]
    }
   ],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "print_results(*model.test(r\"C:\\Users\\Jonas\\Documents\\GSOM\\NLP\\test.ft.txt\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(r\"C:\\Users\\Jonas\\Documents\\GSOM\\NLP\\test.ft.txt\",sep=\"\\n\",header=None)  \n",
    "train = pd.read_csv(r\"C:\\Users\\Jonas\\Documents\\GSOM\\NLP\\train.ft.txt\",sep=\"\\n\",header=None)\n",
    "\n",
    "\n",
    "test.columns=['text']\n",
    "train.columns=['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1     1800000\n",
      "2     1800000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Make clean Train Dataframe with 2 Columns\n",
    "train_clean= train.copy()\n",
    "train_clean['text']=train_clean['text'].str.replace(\"__label__1\",\"1 \\t\")\n",
    "train_clean['text']=train_clean['text'].str.replace(\"__label__2\",\"2 \\t\")\n",
    "train_clean['label'],train_clean['text'] = train_clean['text'].str.split('\\t').str\n",
    "\n",
    "print(train_clean.groupby('label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1     200000\n",
      "2     200000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Make clean Test Dataframe with 2 Columns\n",
    "test_clean= test.copy()\n",
    "test_clean['text']=test_clean['text'].str.replace(\"__label__1\",\"1 \\t\")\n",
    "test_clean['text']=test_clean['text'].str.replace(\"__label__2\",\"2 \\t\")\n",
    "test_clean['label'],test_clean['text'] = test_clean['text'].str.split('\\t').str\n",
    "\n",
    "print(test_clean.groupby('label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0   Stuning even for the non-gamer: This sound tr...    2 \n",
      "1   The best soundtrack ever to anything.: I'm re...    2 \n",
      "2   Amazing!: This soundtrack is my favorite musi...    2 \n",
      "3   Excellent Soundtrack: I truly like this sound...    2 \n",
      "4   Remember, Pull Your Jaw Off The Floor After H...    2 \n",
      "                                                text label\n",
      "0   Great CD: My lovely Pat has one of the GREAT ...    2 \n",
      "1   One of the best game music soundtracks - for ...    2 \n",
      "2   Batteries died within a year ...: I bought th...    1 \n",
      "3   works fine, but Maha Energy is better: Check ...    2 \n",
      "4   Great for the non-audiophile: Reviewed quite ...    2 \n"
     ]
    }
   ],
   "source": [
    "print(train_clean.head())\n",
    "print(test_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sklearn vectorizer to create the word matrix and dictornary and transform the test data to it. I thought top 50k words will be enough, otherwise the model (espeically normal SVM takes for ever)\n",
    "cv = CountVectorizer(stop_words='english',max_features=50000)\n",
    "\n",
    "ctmTr = cv.fit_transform(train_clean['text'])\n",
    "\n",
    "X_test_dtm = cv.transform(test_clean['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TFiDF to optimze for the SDG classifier\n",
    "tf = TfidfTransformer()\n",
    "\n",
    "trainTF = tf.fit_transform(ctmTr)\n",
    "testTF = tf.transform(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "#Model, first tried with a normal Support Vecotr Machine, but i takes way too long time...\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',verbose=True)\n",
    "SVM.fit(ctmTr,train_clean['label'])\n",
    "\n",
    "#Predict\n",
    "predictions_SVM = SVM.predict(X_test_dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 16.83, NNZs: 50000, Bias: -0.074038, T: 3600000, Avg. loss: 0.557095\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073839, T: 7200000, Avg. loss: 0.556588\n",
      "Total training time: 4.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073326, T: 10800000, Avg. loss: 0.556672\n",
      "Total training time: 7.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073303, T: 14400000, Avg. loss: 0.556686\n",
      "Total training time: 9.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.072680, T: 18000000, Avg. loss: 0.556653\n",
      "Total training time: 11.77 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073183, T: 21600000, Avg. loss: 0.556651\n",
      "Total training time: 14.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073068, T: 25200000, Avg. loss: 0.556638\n",
      "Total training time: 16.56 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073065, T: 28800000, Avg. loss: 0.556642\n",
      "Total training time: 18.84 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073013, T: 32400000, Avg. loss: 0.556637\n",
      "Total training time: 21.24 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073183, T: 36000000, Avg. loss: 0.556645\n",
      "Total training time: 23.58 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 16.82, NNZs: 50000, Bias: -0.073012, T: 39600000, Avg. loss: 0.556636\n",
      "Total training time: 26.01 seconds.\n",
      "Convergence after 11 epochs took 26.01 seconds\n"
     ]
    }
   ],
   "source": [
    "#Model 2 after research on stackoverflow I decided to use the SGDClaffissier from sklearn, which worked much faster and reached a good result\n",
    "SGD= SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3,  max_iter=1000, verbose=2,n_iter_no_change=10)\n",
    "SGD.fit(trainTF,train_clean['label'])  \n",
    "\n",
    "#Predict 2\n",
    "predictions_SGD = SGD.predict(testTF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  84.747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1        0.84      0.86      0.85    200000\n",
      "          2        0.86      0.83      0.84    200000\n",
      "\n",
      "    accuracy                           0.85    400000\n",
      "   macro avg       0.85      0.85      0.85    400000\n",
      "weighted avg       0.85      0.85      0.85    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Even tho is didint improved through the epochs (i tested with 500 it doesnt change) we get good result of 84% Acc:\n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SGD, test_clean['label'])*100)\n",
    "\n",
    "print(classification_report(test_clean['label'], predictions_SGD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network:\n",
    "    \n",
    "#We need vector representations of the sentences. I decided to use keras tokenizer function and a small LSTM Model for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Hyperparamaters for the following functions. I decided to count only top 10.000 words because otherwise neural network takes long time for every epoch.\n",
    "#Also reduced the max sentence lenght to only 100 words and decided for a vector embedding dimension of 100.\n",
    "max_words = 10000\n",
    "max_sent_leng=100\n",
    "embedding_dim=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103004\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer functions to preprocess the data. Fit on Train Data and print the amount of individual tokens.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True,oov_token = True)\n",
    "tokenizer.fit_on_texts(train_clean['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[351, 9, 1023, 10, 22, 298, 118, 8, 28, 56, 801, 164, 2, 2328, 6139, 8, 1, 8, 4963, 89, 896, 28, 2, 46, 29, 965, 529, 2, 145, 3, 2, 398, 11, 5, 6273, 2431, 3, 599, 71, 6382, 2, 304, 21, 88, 42, 675, 2656, 6, 43, 1301, 875, 22, 2247, 138, 2, 1375, 1387, 532, 33, 1, 1892, 1, 1, 56, 8, 2, 1, 3, 1, 1, 1, 8, 1, 4921, 24, 28, 444, 1362, 23, 68, 9, 1023, 10, 351, 118, 312, 2, 87, 8, 9, 1, 92, 4, 586, 315, 2, 1, 1023, 27, 4, 157, 153, 12, 233, 3, 73, 29, 965, 122, 529, 2, 145, 7, 39, 26, 163, 790, 2, 170, 6, 82, 7, 4, 341, 4, 101, 185, 7, 399, 256]\n",
      "(3600000, 100)\n"
     ]
    }
   ],
   "source": [
    "#From Token to int values of the dictonary plus padding of the sentences that are shorter or longer than 100 words.\n",
    "#The number 1 stand for words that are not in the 10k words dictionary created above\n",
    "\n",
    "X_train_nn = tokenizer.texts_to_sequences(train_clean['text'].values)\n",
    "print(X_train_nn[2])\n",
    "X_train_nn = pad_sequences(X_train_nn, maxlen=max_sent_leng)\n",
    "print(X_train_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 100)\n",
      "[   3    7 2573   25    8   22  298  574   57   10   42  970  913    8\n",
      "  216 2611    3 1711  159  186  699    3  367  532    4  353   33   23\n",
      "  508   15   83  111    8  186 2712    8  159   11   22   75  274  145\n",
      " 6652    4  180 1230   13   25    8    2  159  154    5 6273 2431   44\n",
      " 1093 2110    6   22  906   19  111 4970   22   25 1311   41    9 1023\n",
      "   10   13   34   91  697    1  947   11  111    8    2  159   89    4\n",
      "  137 3760   17   73   29  186 1718  754    4   39  127 1045    2  371\n",
      "  163    7]\n"
     ]
    }
   ],
   "source": [
    "#Same for the test data\n",
    "X_test_nn = tokenizer.texts_to_sequences(test_clean['text'].values)\n",
    "X_test_nn = pad_sequences(X_test_nn, maxlen=max_sent_leng)\n",
    "print(X_test_nn.shape)\n",
    "print(X_test_nn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still our train sample it huge with 3.6 mio samples. I decided to reduce the sample size by 50% because of computational / time limits\n",
    "\n",
    "X_train_nn = X_train_nn[:1800000]\n",
    "train_clean_new = train_clean['label']\n",
    "train_clean_new = train_clean_new[:1800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Now create the model:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=X_test_nn.shape[1]))\n",
    "model.add(LSTM(embedding_dim))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonas\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1620000 samples, validate on 180000 samples\n",
      "Epoch 1/5\n",
      "1620000/1620000 [==============================] - 2523s 2ms/step - loss: -7.7042 - accuracy: 0.4975 - val_loss: -7.5797 - val_accuracy: 0.5029\n",
      "Epoch 2/5\n",
      "1620000/1620000 [==============================] - 2358s 1ms/step - loss: -7.7042 - accuracy: 0.4975 - val_loss: -7.5797 - val_accuracy: 0.5029\n",
      "Epoch 3/5\n",
      "1620000/1620000 [==============================] - 2549s 2ms/step - loss: -7.7042 - accuracy: 0.4975 - val_loss: -7.5797 - val_accuracy: 0.5029\n",
      "Epoch 4/5\n",
      "1620000/1620000 [==============================] - 2542s 2ms/step - loss: -7.7042 - accuracy: 0.4975 - val_loss: -7.5797 - val_accuracy: 0.5029\n",
      "Epoch 5/5\n",
      "1620000/1620000 [==============================] - 2549s 2ms/step - loss: -7.7042 - accuracy: 0.4975 - val_loss: -7.5797 - val_accuracy: 0.5029\n"
     ]
    }
   ],
   "source": [
    "#Fit the model. Sadly i had only times for 5 Epochs and a small layered LSTM without dropouts. Therefore the model never really fitted \n",
    "#and did not well at predicting\n",
    "\n",
    "history = model.fit(X_train_nn, train_clean_new, epochs=epochs, batch_size=batch_size,validation_split=0.1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000/400000 [==============================] - 347s 868us/step\n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate(X_test_nn,test_clean['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.624619323925972, 0.5]\n"
     ]
    }
   ],
   "source": [
    "#Model didnt really learn, there for random selecting label 1 or 2 and getting only 50% acc. Models needs a better structure and more epochs\n",
    "#Time and computantional lack to achieve good results\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Conclution: Fasttest scored the highest Acc with 94% followed by standard TFIDF + SGD classifier with 84% and an not well \n",
    "#performing LSTM with 50% acc. Fasttest was also very fast (lol) and worked really well woth its word embedding vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
